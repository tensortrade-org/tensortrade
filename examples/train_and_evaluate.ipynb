{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install TensorTrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install git+https://github.com/abstractguy/tensortrade.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps: int = 1000\n",
    "n_episodes: int = 10\n",
    "save_every: int = None\n",
    "window_size: int = 30\n",
    "memory_capacity: int = n_steps * 10\n",
    "save_path: str = 'agents/'\n",
    "discount_factor: float = 0.95\n",
    "learning_rate: float = 0.01\n",
    "eps_start: float = 0.9\n",
    "eps_end: float =  0.05\n",
    "eps_decay_steps: int = n_steps\n",
    "update_target_every: int = 1000\n",
    "render_interval: int = n_steps // 10\n",
    "n_bins: int = 5             # Number of bins to partition the dataset evenly in order to evaluate class sparsity.\n",
    "seed: int = 1337"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Data Fetching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensortrade'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensortrade\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcdd\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CryptoDataDownload\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensortrade'"
     ]
    }
   ],
   "source": [
    "from tensortrade.data.cdd import CryptoDataDownload\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.mode.use_inf_as_na = True\n",
    "\n",
    "def prepare_data(df):\n",
    "    df['volume'] = np.int64(df['volume'])\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df.sort_values(by='date', ascending=True, inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df['date'] = df['date'].dt.strftime('%Y-%m-%d %I:%M %p')\n",
    "    return df\n",
    "\n",
    "def fetch_data():\n",
    "    cdd = CryptoDataDownload()\n",
    "    bitfinex_data = cdd.fetch(\"Bitfinex\", \"USD\", \"BTC\", \"1h\")\n",
    "    bitfinex_data = bitfinex_data[['date', 'open', 'high', 'low', 'close', 'volume']]\n",
    "    bitfinex_data = prepare_data(bitfinex_data)\n",
    "    return bitfinex_data\n",
    "\n",
    "def load_csv(filename):\n",
    "    df = pd.read_csv('data/' + filename, skiprows=1)\n",
    "    df.drop(columns=['symbol', 'volume_btc'], inplace=True)\n",
    "\n",
    "    # Fix timestamp from \"2019-10-17 09-AM\" to \"2019-10-17 09-00-00 AM\"\n",
    "    df['date'] = df['date'].str[:14] + '00-00 ' + df['date'].str[-2:]\n",
    "\n",
    "    return prepare_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_data()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create features for the feed module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import ta as ta1\n",
    "import pandas_ta as ta\n",
    "\n",
    "import quantstats as qs\n",
    "qs.extend_pandas()\n",
    "\n",
    "def fix_dataset_inconsistencies(dataframe, fill_value=None):\n",
    "    dataframe = dataframe.replace([-np.inf, np.inf], np.nan)\n",
    "\n",
    "    # This is done to avoid filling middle holes with backfilling.\n",
    "    if fill_value is None:\n",
    "        dataframe.iloc[0,:] = \\\n",
    "            dataframe.apply(lambda column: column.iloc[column.first_valid_index()], axis='index')\n",
    "    else:\n",
    "        dataframe.iloc[0,:] = \\\n",
    "            dataframe.iloc[0,:].fillna(fill_value)\n",
    "\n",
    "    return dataframe.fillna(axis='index', method='pad').dropna(axis='columns')\n",
    "\n",
    "def rsi(price: 'pd.Series[pd.Float64Dtype]', period: float) -> 'pd.Series[pd.Float64Dtype]':\n",
    "    r = price.diff()\n",
    "    upside = np.minimum(r, 0).abs()\n",
    "    downside = np.maximum(r, 0).abs()\n",
    "    rs = upside.ewm(alpha=1 / period).mean() / downside.ewm(alpha=1 / period).mean()\n",
    "    return 100*(1 - (1 + rs) ** -1)\n",
    "\n",
    "def macd(price: 'pd.Series[pd.Float64Dtype]', fast: float, slow: float, signal: float) -> 'pd.Series[pd.Float64Dtype]':\n",
    "    fm = price.ewm(span=fast, adjust=False).mean()\n",
    "    sm = price.ewm(span=slow, adjust=False).mean()\n",
    "    md = fm - sm\n",
    "    signal = md - md.ewm(span=signal, adjust=False).mean()\n",
    "    return signal\n",
    "\n",
    "def generate_all_default_quantstats_features(data):\n",
    "    excluded_indicators = [\n",
    "        'compare',\n",
    "        'greeks',\n",
    "        'information_ratio',\n",
    "        'omega',\n",
    "        'r2',\n",
    "        'r_squared',\n",
    "        'rolling_greeks',\n",
    "        'warn',\n",
    "    ]\n",
    "    \n",
    "    indicators_list = [f for f in dir(qs.stats) if f[0] != '_' and f not in excluded_indicators]\n",
    "    \n",
    "    df = data.copy()\n",
    "    df = df.set_index('date')\n",
    "    df.index = pd.DatetimeIndex(df.index)\n",
    "\n",
    "    for indicator_name in indicators_list:\n",
    "        try:\n",
    "            #print(indicator_name)\n",
    "            indicator = qs.stats.__dict__[indicator_name](df['close'])\n",
    "            if isinstance(indicator, pd.Series):\n",
    "                indicator = indicator.to_frame(name=indicator_name)\n",
    "                df = pd.concat([df, indicator], axis='columns')\n",
    "        except (pd.errors.InvalidIndexError, ValueError):\n",
    "            pass\n",
    "\n",
    "    df = df.reset_index()\n",
    "    return df\n",
    "\n",
    "def generate_features(data):\n",
    "    # Automatically-generated using pandas_ta\n",
    "    df = data.copy()\n",
    "\n",
    "    strategies = ['candles', \n",
    "                  'cycles', \n",
    "                  'momentum', \n",
    "                  'overlap', \n",
    "                  'performance', \n",
    "                  'statistics', \n",
    "                  'trend', \n",
    "                  'volatility', \n",
    "                  'volume']\n",
    "\n",
    "    df.index = pd.DatetimeIndex(df.index)\n",
    "\n",
    "    cores = os.cpu_count()\n",
    "    df.ta.cores = cores\n",
    "\n",
    "    for strategy in strategies:\n",
    "        df.ta.study(strategy, exclude=['kvo'])\n",
    "\n",
    "    df = df.set_index('date')\n",
    "\n",
    "    # Generate all default indicators from ta library\n",
    "    ta1.add_all_ta_features(data, \n",
    "                            'open', \n",
    "                            'high', \n",
    "                            'low', \n",
    "                            'close', \n",
    "                            'volume', \n",
    "                            fillna=True)\n",
    "\n",
    "    # Naming convention across most technical indicator libraries\n",
    "    data = data.rename(columns={'open': 'Open', \n",
    "                                'high': 'High', \n",
    "                                'low': 'Low', \n",
    "                                'close': 'Close', \n",
    "                                'volume': 'Volume'})\n",
    "    data = data.set_index('date')\n",
    "\n",
    "    # Custom indicators\n",
    "    features = pd.DataFrame.from_dict({\n",
    "        'prev_open': data['Open'].shift(1),\n",
    "        'prev_high': data['High'].shift(1),\n",
    "        'prev_low': data['Low'].shift(1),\n",
    "        'prev_close': data['Close'].shift(1),\n",
    "        'prev_volume': data['Volume'].shift(1),\n",
    "        'vol_5': data['Close'].rolling(window=5).std().abs(),\n",
    "        'vol_10': data['Close'].rolling(window=10).std().abs(),\n",
    "        'vol_20': data['Close'].rolling(window=20).std().abs(),\n",
    "        'vol_30': data['Close'].rolling(window=30).std().abs(),\n",
    "        'vol_50': data['Close'].rolling(window=50).std().abs(),\n",
    "        'vol_60': data['Close'].rolling(window=60).std().abs(),\n",
    "        'vol_100': data['Close'].rolling(window=100).std().abs(),\n",
    "        'vol_200': data['Close'].rolling(window=200).std().abs(),\n",
    "        'ma_5': data['Close'].rolling(window=5).mean(),\n",
    "        'ma_10': data['Close'].rolling(window=10).mean(),\n",
    "        'ma_20': data['Close'].rolling(window=20).mean(),\n",
    "        'ma_30': data['Close'].rolling(window=30).mean(),\n",
    "        'ma_50': data['Close'].rolling(window=50).mean(),\n",
    "        'ma_60': data['Close'].rolling(window=60).mean(),\n",
    "        'ma_100': data['Close'].rolling(window=100).mean(),\n",
    "        'ma_200': data['Close'].rolling(window=200).mean(),\n",
    "        'ema_5': ta1.trend.ema_indicator(data['Close'], window=5, fillna=True),\n",
    "        'ema_10': ta1.trend.ema_indicator(data['Close'], window=10, fillna=True),\n",
    "        'ema_20': ta1.trend.ema_indicator(data['Close'], window=20, fillna=True),\n",
    "        'ema_60': ta1.trend.ema_indicator(data['Close'], window=60, fillna=True),\n",
    "        'ema_64': ta1.trend.ema_indicator(data['Close'], window=64, fillna=True),\n",
    "        'ema_120': ta1.trend.ema_indicator(data['Close'], window=120, fillna=True),\n",
    "        'lr_open': np.log(data['Open']).diff().fillna(0),\n",
    "        'lr_high': np.log(data['High']).diff().fillna(0),\n",
    "        'lr_low': np.log(data['Low']).diff().fillna(0),\n",
    "        'lr_close': np.log(data['Close']).diff().fillna(0),\n",
    "        'r_volume': data['Close'].diff().fillna(0),\n",
    "        'rsi_5': rsi(data['Close'], period=5),\n",
    "        'rsi_10': rsi(data['Close'], period=10),\n",
    "        'rsi_100': rsi(data['Close'], period=100),\n",
    "        'rsi_7': rsi(data['Close'], period=7),\n",
    "        'rsi_28': rsi(data['Close'], period=28),\n",
    "        'rsi_6': rsi(data['Close'], period=6),\n",
    "        'rsi_14': rsi(data['Close'], period=14),\n",
    "        'rsi_26': rsi(data['Close'], period=24),\n",
    "        'macd_normal': macd(data['Close'], fast=12, slow=26, signal=9),\n",
    "        'macd_short': macd(data['Close'], fast=10, slow=50, signal=5),\n",
    "        'macd_long': macd(data['Close'], fast=200, slow=100, signal=50),\n",
    "    })\n",
    "\n",
    "    # Concatenate both manually and automatically generated features\n",
    "    data = pd.concat([data, features], axis='columns').fillna(method='pad')\n",
    "\n",
    "    # Remove potential column duplicates\n",
    "    data = data.loc[:,~data.columns.duplicated()]\n",
    "\n",
    "    # Revert naming convention\n",
    "    data = data.rename(columns={'Open': 'open', \n",
    "                                'High': 'high', \n",
    "                                'Low': 'low', \n",
    "                                'Close': 'close', \n",
    "                                'Volume': 'volume'})\n",
    "\n",
    "    # Concatenate both manually and automatically generated features\n",
    "    data = pd.concat([data, df], axis='columns').fillna(method='pad')\n",
    "\n",
    "    # Remove potential column duplicates\n",
    "    data = data.loc[:,~data.columns.duplicated()]\n",
    "\n",
    "    data = data.reset_index()\n",
    "\n",
    "    # Generate all default quantstats features\n",
    "    df_quantstats = generate_all_default_quantstats_features(data)\n",
    "\n",
    "    # Concatenate both manually and automatically generated features\n",
    "    data = pd.concat([data, df_quantstats], axis='columns').fillna(method='pad')\n",
    "\n",
    "    # Remove potential column duplicates\n",
    "    data = data.loc[:,~data.columns.duplicated()]\n",
    "\n",
    "    # A lot of indicators generate NaNs at the beginning of DataFrames, so remove them\n",
    "    data = data.iloc[200:]\n",
    "    data = data.reset_index(drop=True)\n",
    "\n",
    "    data = fix_dataset_inconsistencies(data, fill_value=None)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = generate_features(data)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop redundant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=['PCTRET_1', \n",
    "                          'LOGRET_1', \n",
    "                          'lr_close', \n",
    "                          'others_dr', \n",
    "                          'others_dlr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove features with low variance before splitting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "sel = VarianceThreshold(threshold=0.0)\n",
    "date = data[['date']].copy()\n",
    "data = data.drop(columns=['date'])\n",
    "sel.fit(data)\n",
    "data[data.columns[sel.get_support(indices=True)]]\n",
    "data = pd.concat([date, data], axis='columns')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_data(data):\n",
    "    X = data.copy()\n",
    "    y = X['close'].pct_change()\n",
    "\n",
    "    X_train_test, X_valid, y_train_test, y_valid = \\\n",
    "        train_test_split(data, data['close'].pct_change(), train_size=0.67, test_size=0.33, shuffle=False)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(X_train_test, y_train_test, train_size=0.50, test_size=0.50, shuffle=False)\n",
    "\n",
    "    return X_train, X_test, X_valid, y_train, y_test, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, X_valid, y_train, y_test, y_valid = \\\n",
    "    split_data(data)\n",
    "\n",
    "import os\n",
    "cwd = os.getcwd()\n",
    "train_csv = os.path.join(cwd, 'train.csv')\n",
    "test_csv = os.path.join(cwd, 'test.csv')\n",
    "valid_csv = os.path.join(cwd, 'valid.csv')\n",
    "X_train.to_csv(train_csv, index=False)\n",
    "X_test.to_csv(test_csv, index=False)\n",
    "X_valid.to_csv(valid_csv, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get dataset statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import iqr\n",
    "\n",
    "def estimate_outliers(data):\n",
    "    return iqr(data) * 1.5\n",
    "\n",
    "def estimate_percent_gains(data, column='close'):\n",
    "    returns = get_returns(data, column=column)\n",
    "    gains = estimate_outliers(returns)\n",
    "    return gains\n",
    "\n",
    "def get_returns(data, column='close'):\n",
    "    return fix_dataset_inconsistencies(data[[column]].pct_change(), fill_value=0)\n",
    "\n",
    "def precalculate_ground_truths(data, column='close', threshold=None):\n",
    "    returns = get_returns(data, column=column)\n",
    "    gains = estimate_outliers(returns) if threshold is None else threshold\n",
    "    binary_gains = (returns[column] > gains).astype(int)\n",
    "    return binary_gains\n",
    "\n",
    "def is_null(data):\n",
    "    return data.isnull().sum().sum() > 0\n",
    "\n",
    "def is_sparse(data, column='close'):\n",
    "    binary_gains = precalculate_ground_truths(data, column=column)\n",
    "    bins = [n * (binary_gains.shape[0] // n_bins) for n in range(n_bins)]\n",
    "    bins += [binary_gains.shape[0]]\n",
    "    bins = [binary_gains.iloc[bins[n]:bins[n + 1]] for n in range(n_bins)]\n",
    "    return all([bin.astype(bool).any() for bin in bins])\n",
    "\n",
    "def is_data_predictible(data, column):\n",
    "    return not is_null(data) & is_sparse(data, column)\n",
    "\n",
    "data.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate outlier sparsity of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(get_returns(data, column='close'))\n",
    "plt.show()\n",
    "is_data_predictible(data, 'close')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Percentage of the dataset generating rewards (keep between 5% to 15% or just rely on is_data_predictible())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(precalculate_ground_truths(data, column='close').iloc[:1000])\n",
    "plt.show()\n",
    "percent_rewardable = str(round(100 + precalculate_ground_truths(data, column='close').value_counts().pct_change().iloc[-1] * 100, 2)) + '%'\n",
    "print(percent_rewardable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threshold to pass to AnomalousProfit reward scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_test = pd.concat([X_train, X_test], axis='index')\n",
    "#threshold = estimate_percent_gains(X_train_test, 'close')\n",
    "threshold = estimate_percent_gains(X_train, 'close')\n",
    "threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement basic feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade feature_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from feature_engine.selection import SelectBySingleFeaturePerformance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100, \n",
    "                            random_state=seed, \n",
    "                            n_jobs=6)\n",
    "\n",
    "sel = SelectBySingleFeaturePerformance(variables=None, \n",
    "                                       estimator=rf, \n",
    "                                       scoring=\"roc_auc\", \n",
    "                                       cv=5, \n",
    "                                       threshold=0.6)\n",
    "\n",
    "sel.fit(X_train, precalculate_ground_truths(X_train, column='close'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_performance = pd.Series(sel.feature_performance_).sort_values(ascending=False)\n",
    "feature_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_performance.plot.bar(figsize=(20, 5))\n",
    "plt.title('Performance of ML models trained with individual features')\n",
    "plt.ylabel('roc-auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_drop = sel.features_to_drop_\n",
    "features_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = list(set(features_to_drop) - set(['open', 'high', 'low', 'close', 'volume']))\n",
    "len(to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop(columns=to_drop)\n",
    "X_test = X_test.drop(columns=to_drop)\n",
    "X_valid = X_valid.drop(columns=to_drop)\n",
    "\n",
    "X_train.shape, X_test.shape, X_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize the dataset subsets to make the model converge faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler\n",
    "\n",
    "scaler_type = MinMaxScaler\n",
    "\n",
    "def get_feature_scalers(X, scaler_type=scaler_type):\n",
    "    scalers = []\n",
    "    for name in list(X.columns[X.columns != 'date']):\n",
    "        scalers.append(scaler_type().fit(X[name].values.reshape(-1, 1)))\n",
    "    return scalers\n",
    "\n",
    "def get_scaler_transforms(X, scalers):\n",
    "    X_scaled = []\n",
    "    for name, scaler in zip(list(X.columns[X.columns != 'date']), scalers):\n",
    "        X_scaled.append(scaler.transform(X[name].values.reshape(-1, 1)))\n",
    "    X_scaled = pd.concat([pd.DataFrame(column, columns=[name]) for name, column in \\\n",
    "                          zip(list(X.columns[X.columns != 'date']), X_scaled)], axis='columns')\n",
    "    return X_scaled\n",
    "\n",
    "def normalize_data(X_train, X_test, X_valid):\n",
    "    X_train_test = pd.concat([X_train, X_test], axis='index')\n",
    "    X_train_test_valid = pd.concat([X_train_test, X_valid], axis='index')\n",
    "\n",
    "    X_train_test_dates = X_train_test[['date']]\n",
    "    X_train_test_valid_dates = X_train_test_valid[['date']]\n",
    "\n",
    "    X_train_test = X_train_test.drop(columns=['date'])\n",
    "    X_train_test_valid = X_train_test_valid.drop(columns=['date'])\n",
    "\n",
    "    train_test_scalers = \\\n",
    "        get_feature_scalers(X_train_test, \n",
    "                            scaler_type=scaler_type)\n",
    "    train_test_valid_scalers = \\\n",
    "        get_feature_scalers(X_train_test_valid, \n",
    "                            scaler_type=scaler_type)\n",
    "\n",
    "    X_train_test_scaled = \\\n",
    "        get_scaler_transforms(X_train_test, \n",
    "                              train_test_scalers)\n",
    "    X_train_test_valid_scaled = \\\n",
    "        get_scaler_transforms(X_train_test_valid, \n",
    "                              train_test_scalers)\n",
    "    X_train_test_valid_scaled_leaking = \\\n",
    "        get_scaler_transforms(X_train_test_valid, \n",
    "                              train_test_valid_scalers)\n",
    "\n",
    "    X_train_test_scaled = \\\n",
    "        pd.concat([X_train_test_dates, \n",
    "                   X_train_test_scaled], \n",
    "                  axis='columns')\n",
    "    X_train_test_valid_scaled = \\\n",
    "        pd.concat([X_train_test_valid_dates, \n",
    "                   X_train_test_valid_scaled], \n",
    "                  axis='columns')\n",
    "    X_train_test_valid_scaled_leaking = \\\n",
    "        pd.concat([X_train_test_valid_dates, \n",
    "                   X_train_test_valid_scaled_leaking], \n",
    "                  axis='columns')\n",
    "\n",
    "    X_train_scaled = X_train_test_scaled.iloc[:X_train.shape[0]]\n",
    "    X_test_scaled = X_train_test_scaled.iloc[X_train.shape[0]:]\n",
    "    X_valid_scaled = X_train_test_valid_scaled.iloc[X_train_test.shape[0]:]\n",
    "    X_valid_scaled_leaking = X_train_test_valid_scaled_leaking.iloc[X_train_test.shape[0]:]\n",
    "\n",
    "    return (train_test_scalers, \n",
    "            train_test_valid_scalers, \n",
    "            X_train_scaled, \n",
    "            X_test_scaled, \n",
    "            X_valid_scaled, \n",
    "            X_valid_scaled_leaking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_scalers, train_test_valid_scalers, X_train_scaled, X_test_scaled, X_valid_scaled, X_valid_scaled_leaking = \\\n",
    "    normalize_data(X_train, X_test, X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## There are more useful features than OHLCV to use for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = X_train_scaled.drop(columns=['open', \n",
    "                                              'high', \n",
    "                                              'low', \n",
    "                                              'close', \n",
    "                                              'volume'])\n",
    "\n",
    "X_test_scaled = X_test_scaled.drop(columns=['open', \n",
    "                                            'high', \n",
    "                                            'low', \n",
    "                                            'close', \n",
    "                                            'volume'])\n",
    "\n",
    "X_valid_scaled = X_valid_scaled.drop(columns=['open', \n",
    "                                              'high', \n",
    "                                              'low', \n",
    "                                              'close', \n",
    "                                              'volume'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write a reward scheme encouraging rare volatile upside trades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensortrade.env.default.rewards import TensorTradeRewardScheme\n",
    "\n",
    "\n",
    "class AnomalousProfit(TensorTradeRewardScheme):\n",
    "    \"\"\"A simple reward scheme that rewards the agent for exceeding a \n",
    "    precalculated percentage in the net worth.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    threshold : float\n",
    "        The minimum value to exceed in order to get the reward.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    threshold : float\n",
    "        The minimum value to exceed in order to get the reward.\n",
    "    \"\"\"\n",
    "\n",
    "    registered_name = \"anomalous\"\n",
    "\n",
    "    def __init__(self, threshold: float = 0.02, window_size: int = 1):\n",
    "        self._window_size = self.default('window_size', window_size)\n",
    "        self._threshold = self.default('threshold', threshold)\n",
    "\n",
    "    def get_reward(self, portfolio: 'Portfolio') -> float:\n",
    "        \"\"\"Rewards the agent for incremental increases in net worth over a\n",
    "        sliding window.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        portfolio : `Portfolio`\n",
    "            The portfolio being used by the environment.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            Whether the last percent change in net worth exceeds the predefined \n",
    "            `threshold`.\n",
    "        \"\"\"\n",
    "        performance = pd.DataFrame.from_dict(portfolio.performance).T\n",
    "        current_step = performance.shape[0]\n",
    "        if current_step > 1:\n",
    "            # Hint: make it cumulative.\n",
    "            net_worths = performance['net_worth']\n",
    "            ground_truths = precalculate_ground_truths(performance, \n",
    "                                                       column='net_worth', \n",
    "                                                       threshold=self._threshold)\n",
    "            reward_factor = 2.0 * ground_truths - 1.0\n",
    "            #return net_worths.iloc[-1] / net_worths.iloc[-min(current_step, self._window_size + 1)] - 1.0\n",
    "            return (reward_factor * net_worths.abs()).iloc[-1]\n",
    "\n",
    "        else:\n",
    "            return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PenalizedProfit(TensorTradeRewardScheme):\n",
    "    \"\"\"A reward scheme which penalizes net worth loss and \n",
    "    decays with the time spent.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cash_penalty_proportion : float\n",
    "        cash_penalty_proportion\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    cash_penalty_proportion : float\n",
    "        cash_penalty_proportion.\n",
    "    \"\"\"\n",
    "\n",
    "    registered_name = \"penalized\"\n",
    "\n",
    "    def __init__(self, cash_penalty_proportion: float = 0.10):\n",
    "        self._cash_penalty_proportion = \\\n",
    "            self.default('cash_penalty_proportion', \n",
    "                         cash_penalty_proportion)\n",
    "\n",
    "    def get_reward(self, portfolio: 'Portfolio') -> float:\n",
    "        \"\"\"Rewards the agent for gaining net worth while holding the asset.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        portfolio : `Portfolio`\n",
    "            The portfolio being used by the environment.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            A penalized reward.\n",
    "        \"\"\"\n",
    "        performance = pd.DataFrame.from_dict(portfolio.performance).T\n",
    "        current_step = performance.shape[0]\n",
    "        if current_step > 1:\n",
    "            initial_amount = portfolio.initial_net_worth\n",
    "            asset_worth = performance['bitstamp:/BTC:/worth'].iloc[-1]\n",
    "            cash_worth = performance['bitstamp:/USD:/total'].iloc[-1]\n",
    "            cash_penalty = max(0, (asset_worth * self._cash_penalty_proportion - cash_worth))\n",
    "            asset_worth -= cash_penalty\n",
    "            reward = (asset_worth / initial_amount) - 1\n",
    "            reward /= current_step\n",
    "            return reward\n",
    "        else:\n",
    "            return 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: implement tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Trading Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensortrade.env.default as default\n",
    "\n",
    "#from tensortrade.agents import DQNAgent\n",
    "from tensortrade.feed.core import DataFeed, Stream\n",
    "from tensortrade.feed.core.base import NameSpace\n",
    "from tensortrade.env.default.actions import BSH, ManagedRiskOrders\n",
    "from tensortrade.env.default.rewards import RiskAdjustedReturns, SimpleProfit\n",
    "from tensortrade.oms.exchanges import Exchange, ExchangeOptions\n",
    "from tensortrade.oms.services.execution.simulated import execute_order\n",
    "from tensortrade.oms.instruments import USD, BTC, ETH\n",
    "from tensortrade.oms.wallets import Wallet, Portfolio\n",
    "from tensortrade.oms.orders import TradeType\n",
    "\n",
    "# TODO: adjust according to your commission percentage, if present\n",
    "commission = 0.001\n",
    "price = Stream.source(list(X_train[\"close\"]), \n",
    "                      dtype=\"float\").rename(\"USD-BTC\")\n",
    "bitstamp_options = ExchangeOptions(commission=commission)\n",
    "bitstamp = Exchange(\"bitstamp\", \n",
    "                    service=execute_order, \n",
    "                    options=bitstamp_options)(price)\n",
    "\n",
    "cash = Wallet(bitstamp, 50000 * USD)\n",
    "asset = Wallet(bitstamp, 0 * BTC)\n",
    "\n",
    "portfolio = Portfolio(USD, [cash, asset])\n",
    "\n",
    "with NameSpace(\"bitstamp\"):\n",
    "    features = [\n",
    "        Stream.source(list(X_train_scaled[c]), \n",
    "                      dtype=\"float\").rename(c) for c in X_train_scaled.columns[1:]\n",
    "        #Stream.source(list(X_train_scaled['lr_close']), dtype=\"float\").rename('lr_close')\n",
    "    ]\n",
    "\n",
    "feed = DataFeed(features)\n",
    "feed.compile()\n",
    "\n",
    "renderer_feed = DataFeed([\n",
    "    Stream.source(list(X_train[\"date\"])).rename(\"date\"),\n",
    "    Stream.source(list(X_train[\"open\"]), dtype=\"float\").rename(\"open\"),\n",
    "    Stream.source(list(X_train[\"high\"]), dtype=\"float\").rename(\"high\"),\n",
    "    Stream.source(list(X_train[\"low\"]), dtype=\"float\").rename(\"low\"),\n",
    "    Stream.source(list(X_train[\"close\"]), dtype=\"float\").rename(\"close\"), \n",
    "    Stream.source(list(X_train[\"volume\"]), dtype=\"float\").rename(\"volume\") \n",
    "])\n",
    "\n",
    "action_scheme = BSH(\n",
    "    cash=cash,\n",
    "    asset=asset\n",
    ")\n",
    "\n",
    "#reward_scheme = RiskAdjustedReturns(return_algorithm='sortino',\n",
    "#                                    window_size=30)\n",
    "\n",
    "#reward_scheme = SimpleProfit(window_size=30)\n",
    "\n",
    "#reward_scheme = AnomalousProfit(threshold=threshold)\n",
    "\n",
    "reward_scheme = PenalizedProfit(cash_penalty_proportion=0.1)\n",
    "\n",
    "env = default.create(\n",
    "    portfolio=portfolio,\n",
    "    action_scheme=action_scheme,\n",
    "    reward_scheme=reward_scheme,\n",
    "    feed=feed,\n",
    "    renderer_feed=renderer_feed,\n",
    "    renderer=default.renderers.PlotlyTradingChart(),\n",
    "    window_size=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /usr/local/lib/python3.8/dist-packages/tensortrade/agents/a2c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensortrade.agents.a2c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observer.feed.next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Train DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimal_batch_size(window_size=30, n_steps=1000, batch_factor=4, stride=1):\n",
    "    \"\"\"\n",
    "    lookback = 30          # Days of past data (also named window_size).\n",
    "    batch_factor = 4       # batch_size = (sample_size - lookback - stride) // batch_factor\n",
    "    stride = 1             # Time series shift into the future.\n",
    "    \"\"\"\n",
    "    lookback = window_size\n",
    "    sample_size = n_steps\n",
    "    batch_size = ((sample_size - lookback - stride) // batch_factor)\n",
    "    return batch_size\n",
    "\n",
    "batch_size = get_optimal_batch_size(window_size=window_size, n_steps=n_steps, batch_factor=4)\n",
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#agent = DQNAgent(env)\n",
    "#\n",
    "#agent.train(batch_size=batch_size, \n",
    "#            n_steps=n_steps, \n",
    "#            n_episodes=n_episodes, \n",
    "#            memory_capacity=memory_capacity, \n",
    "#            save_path=save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement validation here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print basic quantstats report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_quantstats_full_report(env, data, output='dqn_quantstats'):\n",
    "    performance = pd.DataFrame.from_dict(env.action_scheme.portfolio.performance, orient='index')\n",
    "    net_worth = performance['net_worth'].iloc[window_size:]\n",
    "    returns = net_worth.pct_change().iloc[1:]\n",
    "\n",
    "    # WARNING! The dates are fake and default parameters are used!\n",
    "    returns.index = pd.date_range(start=data['date'].iloc[0], freq='1d', periods=returns.size)\n",
    "\n",
    "    qs.reports.full(returns)\n",
    "    qs.reports.html(returns, output=output + '.html')\n",
    "\n",
    "#print_quantstats_full_report(env, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_allowed_loss = 0.90\n",
    "min_periods = window_size  # Minimum of window_size\n",
    "\n",
    "observer = default.observers.TensorTradeObserver(\n",
    "    portfolio=portfolio,\n",
    "    feed=feed,\n",
    "    renderer_feed=renderer_feed,\n",
    "    window_size=window_size,\n",
    "    min_periods=min_periods\n",
    ")\n",
    "\n",
    "stopper = default.stoppers.MaxLossStopper(\n",
    "    max_allowed_loss=max_allowed_loss\n",
    ")\n",
    "\n",
    "informer = default.informers.TensorTradeInformer()\n",
    "\n",
    "renderer = default.renderers.PlotlyTradingChart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!apt update\n",
    "#!apt install -y swig cmake libz-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r requirements.agents.txt\n",
    "#!yes | pip uninstall opencv-python\n",
    "#!pip install opencv-python\n",
    "#!pip install --upgrade gym==0.21.0 gym-wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.envs.register(\n",
    "    id='TradingEnv-v0',\n",
    "    entry_point='tensortrade.env.generic.environment:TradingEnv',\n",
    "    kwargs={\n",
    "        'portfolio': portfolio,\n",
    "        'action_scheme': action_scheme,\n",
    "        'reward_scheme': reward_scheme,\n",
    "        'observer': observer,\n",
    "        'stopper': stopper,\n",
    "        'informer': informer,\n",
    "        'feed': feed,\n",
    "        'renderer_feed': renderer_feed,\n",
    "        'renderer': renderer,\n",
    "        'min_periods': min_periods,\n",
    "        'window_size': window_size,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensortrade.agents as agents\n",
    "from tensortrade.agents import DQN\n",
    "from tensortrade.agents.utils.common import ModelReader, create_envs\n",
    "\n",
    "envs = create_envs('TradingEnv-v0', preprocess=False, model='models/')\n",
    "model = ModelReader(\n",
    "    tensortrade.agents.agents['dqn']['model']['cnn'][0],\n",
    "    output_units=[1],\n",
    "    input_shape=envs[0].observation_space.shape,\n",
    "    optimizer='adam',\n",
    ").build_model()\n",
    "\n",
    "model.load_weights(\n",
    "    'agents/trained-weights.tf'\n",
    ").expect_partial()\n",
    "\n",
    "agent = A2C(envs, model)\n",
    "agent.play(render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
