{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install TensorTrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pip install git+https://github.com/tensortrade-org/tensortrade.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 1000\n",
    "n_episodes = 20\n",
    "window_size = 30\n",
    "memory_capacity = n_steps * 10\n",
    "learning_rate = 0.0003\n",
    "total_timesteps = 500000\n",
    "save_path = 'agents/'\n",
    "n_bins = 5             # Number of bins to partition the dataset evenly in order to evaluate class sparsity.\n",
    "seed = 1337"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup data fetching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensortrade.data.cdd import CryptoDataDownload\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.mode.use_inf_as_na = True\n",
    "\n",
    "def prepare_data(df):\n",
    "    df['volume'] = np.int64(df['volume'])\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df.sort_values(by='date', ascending=True, inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df['date'] = df['date'].dt.strftime('%Y-%m-%d %I:%M %p')\n",
    "    return df\n",
    "\n",
    "def fetch_data():\n",
    "    cdd = CryptoDataDownload()\n",
    "    bitfinex_data = cdd.fetch(\"Bitfinex\", \"USD\", \"BTC\", \"1h\")\n",
    "    bitfinex_data = bitfinex_data[['date', 'open', 'high', 'low', 'close', 'volume']]\n",
    "    return bitfinex_data\n",
    "\n",
    "def load_csv(filename):\n",
    "    df = pd.read_csv('data/' + filename, skiprows=1)\n",
    "    df.drop(columns=['symbol', 'volume_btc'], inplace=True)\n",
    "\n",
    "    # Fix timestamp from \"2019-10-17 09-AM\" to \"2019-10-17 09-00-00 AM\"\n",
    "    df['date'] = df['date'].str[:14] + '00-00 ' + df['date'].str[-2:]\n",
    "\n",
    "    return prepare_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_data()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import yfinance as yf\n",
    "\n",
    "main_ticker = 'BTC-USD'  # TODO: replace this with your own ticker\n",
    "\n",
    "ticker = yf.Ticker(ticker=main_ticker)\n",
    "\n",
    "data = ticker.history(period='max', interval='1d')\n",
    "data = data[['Open', 'High', 'Low', 'Close', 'Volume']]\n",
    "data['Volume'] = data['Volume'].astype(int)\n",
    "data = data.reset_index()\n",
    "data = data.rename(columns={'Open': 'open', \n",
    "                            'High': 'high', \n",
    "                            'Low': 'low', \n",
    "                            'Close': 'close', \n",
    "                            'Volume': 'volume', \n",
    "                            'Date': 'date'})\n",
    "#data = prepare_data(data)\n",
    "data\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create features for the feed module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import ta as ta1\n",
    "import pandas_ta as ta\n",
    "\n",
    "import quantstats as qs\n",
    "qs.extend_pandas()\n",
    "\n",
    "def fix_dataset_inconsistencies(dataframe, fill_value=None):\n",
    "    dataframe = dataframe.replace([-np.inf, np.inf], np.nan)\n",
    "\n",
    "    # This is done to avoid filling middle holes with backfilling.\n",
    "    if fill_value is None:\n",
    "        dataframe.iloc[0,:] = \\\n",
    "            dataframe.apply(lambda column: column.iloc[column.first_valid_index()], axis='index')\n",
    "    else:\n",
    "        dataframe.iloc[0,:] = \\\n",
    "            dataframe.iloc[0,:].fillna(fill_value)\n",
    "\n",
    "    return dataframe.fillna(axis='index', method='pad').dropna(axis='columns')\n",
    "\n",
    "def rsi(price: 'pd.Series[pd.Float64Dtype]', period: float) -> 'pd.Series[pd.Float64Dtype]':\n",
    "    r = price.diff()\n",
    "    upside = np.minimum(r, 0).abs()\n",
    "    downside = np.maximum(r, 0).abs()\n",
    "    rs = upside.ewm(alpha=1 / period).mean() / downside.ewm(alpha=1 / period).mean()\n",
    "    return 100*(1 - (1 + rs) ** -1)\n",
    "\n",
    "def macd(price: 'pd.Series[pd.Float64Dtype]', fast: float, slow: float, signal: float) -> 'pd.Series[pd.Float64Dtype]':\n",
    "    fm = price.ewm(span=fast, adjust=False).mean()\n",
    "    sm = price.ewm(span=slow, adjust=False).mean()\n",
    "    md = fm - sm\n",
    "    signal = md - md.ewm(span=signal, adjust=False).mean()\n",
    "    return signal\n",
    "\n",
    "def generate_all_pandas_ta_features(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Generate all default indicators from the pandas_ta library\"\"\"\n",
    "    df = data.copy()\n",
    "    \n",
    "    strategies = ['candles', \n",
    "                  'cycles', \n",
    "                  'momentum', \n",
    "                  'overlap', \n",
    "                  'performance', \n",
    "                  'statistics', \n",
    "                  'trend', \n",
    "                  'volatility', \n",
    "                  'volume']\n",
    "    \n",
    "    df.index = pd.DatetimeIndex(df.index)\n",
    "    \n",
    "    cores = os.cpu_count()\n",
    "    df.ta.cores = cores\n",
    "    \n",
    "    for strategy in strategies:\n",
    "        df.ta.strategy(strategy, exclude=['kvo'])\n",
    "\n",
    "    #df = df.set_index('date')\n",
    "\n",
    "    return df\n",
    "\n",
    "def generate_all_ta_features(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Generate all default indicators from the ta library\"\"\"\n",
    "    df = data.copy()\n",
    "\n",
    "    ta1.add_all_ta_features(df, \n",
    "                            'open', \n",
    "                            'high', \n",
    "                            'low', \n",
    "                            'close', \n",
    "                            'volume', \n",
    "                            fillna=True)\n",
    "\n",
    "    #df = df.set_index('date')\n",
    "\n",
    "    return df\n",
    "\n",
    "def generate_all_custom_features(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Generate all custom indicators\"\"\"\n",
    "\n",
    "    df = data.copy()\n",
    "    df_indexes = df[['date']]\n",
    "\n",
    "    # Custom indicators\n",
    "    df = pd.DataFrame.from_dict({\n",
    "        'prev_open': df['open'].shift(1),\n",
    "        'prev_high': df['high'].shift(1),\n",
    "        'prev_low': df['low'].shift(1),\n",
    "        'prev_close': df['close'].shift(1),\n",
    "        'prev_volume': df['volume'].shift(1),\n",
    "        'vol_5': df['close'].rolling(window=5).std().abs(),\n",
    "        'vol_10': df['close'].rolling(window=10).std().abs(),\n",
    "        'vol_20': df['close'].rolling(window=20).std().abs(),\n",
    "        'vol_30': df['close'].rolling(window=30).std().abs(),\n",
    "        'vol_50': df['close'].rolling(window=50).std().abs(),\n",
    "        'vol_60': df['close'].rolling(window=60).std().abs(),\n",
    "        'vol_100': df['close'].rolling(window=100).std().abs(),\n",
    "        'vol_200': df['close'].rolling(window=200).std().abs(),\n",
    "        'ma_5': df['close'].rolling(window=5).mean(),\n",
    "        'ma_10': df['close'].rolling(window=10).mean(),\n",
    "        'ma_20': df['close'].rolling(window=20).mean(),\n",
    "        'ma_30': df['close'].rolling(window=30).mean(),\n",
    "        'ma_50': df['close'].rolling(window=50).mean(),\n",
    "        'ma_60': df['close'].rolling(window=60).mean(),\n",
    "        'ma_100': df['close'].rolling(window=100).mean(),\n",
    "        'ma_200': df['close'].rolling(window=200).mean(),\n",
    "        'ema_5': ta1.trend.ema_indicator(df['close'], window=5, fillna=True),\n",
    "        'ema_10': ta1.trend.ema_indicator(df['close'], window=10, fillna=True),\n",
    "        'ema_20': ta1.trend.ema_indicator(df['close'], window=20, fillna=True),\n",
    "        'ema_60': ta1.trend.ema_indicator(df['close'], window=60, fillna=True),\n",
    "        'ema_64': ta1.trend.ema_indicator(df['close'], window=64, fillna=True),\n",
    "        'ema_120': ta1.trend.ema_indicator(df['close'], window=120, fillna=True),\n",
    "        'lr_open': np.log(df['open']).diff().fillna(0),\n",
    "        'lr_high': np.log(df['high']).diff().fillna(0),\n",
    "        'lr_low': np.log(df['low']).diff().fillna(0),\n",
    "        'lr_close': np.log(df['close']).diff().fillna(0),\n",
    "        'r_volume': df['close'].diff().fillna(0),\n",
    "        'rsi_5': rsi(df['close'], period=5),\n",
    "        'rsi_10': rsi(df['close'], period=10),\n",
    "        'rsi_100': rsi(df['close'], period=100),\n",
    "        'rsi_7': rsi(df['close'], period=7),\n",
    "        'rsi_28': rsi(df['close'], period=28),\n",
    "        'rsi_6': rsi(df['close'], period=6),\n",
    "        'rsi_14': rsi(df['close'], period=14),\n",
    "        'rsi_24': rsi(df['close'], period=24),\n",
    "        'macd_normal': macd(df['close'], fast=12, slow=26, signal=9),\n",
    "        'macd_short': macd(df['close'], fast=10, slow=50, signal=5),\n",
    "        'macd_long': macd(df['close'], fast=200, slow=100, signal=50),\n",
    "    })\n",
    "\n",
    "    df = pd.concat([df_indexes, df], axis='columns')\n",
    "    #df = df.set_index('date')\n",
    "\n",
    "    return df\n",
    "\n",
    "def generate_all_quantstats_features(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Generate all default indicators from the quantstats library\"\"\"\n",
    "    excluded_indicators = [\n",
    "        'compare',\n",
    "        'greeks',\n",
    "        'information_ratio',\n",
    "        'omega',\n",
    "        'r2',\n",
    "        'r_squared',\n",
    "        'rolling_greeks',\n",
    "        'warn',\n",
    "    ]\n",
    "    \n",
    "    indicators_list = [f for f in dir(qs.stats) if f[0] != '_' and f not in excluded_indicators]\n",
    "    \n",
    "    df = data.copy()\n",
    "    df = df.set_index('date')\n",
    "    df.index = pd.DatetimeIndex(df.index)\n",
    "\n",
    "    for indicator_name in indicators_list:\n",
    "        try:\n",
    "            #print(indicator_name)\n",
    "            indicator = qs.stats.__dict__[indicator_name](df['close'])\n",
    "            if isinstance(indicator, pd.Series):\n",
    "                indicator = indicator.to_frame(name=indicator_name)\n",
    "                df = pd.concat([df, indicator], axis='columns')\n",
    "        except (pd.errors.InvalidIndexError, ValueError):\n",
    "            pass\n",
    "\n",
    "    df = df.reset_index()\n",
    "\n",
    "    return df\n",
    "\n",
    "def generate_features(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_pandas_ta = generate_all_pandas_ta_features(data)\n",
    "    df_ta = generate_all_ta_features(data)\n",
    "    df_custom = generate_all_custom_features(data)\n",
    "    df_quantstats = generate_all_quantstats_features(data)\n",
    "\n",
    "    # Remove potential column duplicates\n",
    "    data = data.loc[~data.index.duplicated(),:]\n",
    "\n",
    "    # Concatenate all features\n",
    "    data = pd.concat([df_pandas_ta, df_ta, df_custom, df_quantstats], axis='columns')\n",
    "\n",
    "    # Remove potential column duplicates\n",
    "    data = data.loc[:,~data.columns.duplicated()]\n",
    "\n",
    "    # A lot of indicators generate NaNs at the beginning of DataFrames, so remove them\n",
    "    data = data.iloc[200:]\n",
    "    data = data.reset_index(drop=True)\n",
    "\n",
    "    data = fix_dataset_inconsistencies(data, fill_value=None)\n",
    "\n",
    "    data = data.set_index('date')\n",
    "    data = data[~data.index.duplicated(keep='first')]\n",
    "    data = data.reset_index()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = generate_features(data)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove features with low variance before splitting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
    "date = data[['date']].copy()\n",
    "data = data.drop(columns=['date'])\n",
    "sel.fit(data)\n",
    "data[data.columns[sel.get_support(indices=True)]]\n",
    "data = pd.concat([date, data], axis='columns')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop redundant or broken features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = ['others_dlr', 'compsum']\n",
    "\n",
    "data = data.drop(columns=to_drop)\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_data(data):\n",
    "    X = data.copy()\n",
    "    y = X['close'].pct_change()\n",
    "\n",
    "    X_train_test, X_valid, y_train_test, y_valid = \\\n",
    "        train_test_split(data, data['close'].pct_change(), train_size=0.67, test_size=0.33, shuffle=False)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(X_train_test, y_train_test, train_size=0.50, test_size=0.50, shuffle=False)\n",
    "\n",
    "    return X_train, X_test, X_valid, y_train, y_test, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, X_valid, y_train, y_test, y_valid = \\\n",
    "    split_data(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print basic quantstats report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_quantstats_full_report(env, data, output='dqn_quantstats'):\n",
    "    performance = pd.DataFrame.from_dict(env.action_scheme.portfolio.performance, orient='index')\n",
    "    net_worth = performance['net_worth'].iloc[window_size:]\n",
    "    returns = net_worth.pct_change().iloc[1:]\n",
    "\n",
    "    # WARNING! The dates are fake and default parameters are used!\n",
    "    returns.index = pd.date_range(start=data['date'].iloc[0], freq='1d', periods=returns.size)\n",
    "\n",
    "    qs.reports.full(returns)\n",
    "    qs.reports.html(returns, output=output + '.html')\n",
    "\n",
    "#print_quantstats_full_report(env, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get custom dataset statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import iqr\n",
    "\n",
    "def estimate_outliers(data):\n",
    "    return iqr(data) * 1.5\n",
    "\n",
    "def estimate_percent_gains(data, column='close'):\n",
    "    returns = get_returns(data, column=column)\n",
    "    gains = estimate_outliers(returns)\n",
    "    return gains\n",
    "\n",
    "def get_returns(data, column='close'):\n",
    "    return fix_dataset_inconsistencies(data[[column]].pct_change(), fill_value=0)\n",
    "\n",
    "def precalculate_ground_truths(data, column='close', threshold=None):\n",
    "    returns = get_returns(data, column=column)\n",
    "    gains = estimate_outliers(returns) if threshold is None else threshold\n",
    "    binary_gains = (returns[column] > gains).astype(int)\n",
    "    return binary_gains\n",
    "\n",
    "def is_null(data):\n",
    "    return data.isnull().sum().sum() > 0\n",
    "\n",
    "def is_sparse(data, column='close'):\n",
    "    binary_gains = precalculate_ground_truths(data, column=column)\n",
    "    bins = [n * (binary_gains.shape[0] // n_bins) for n in range(n_bins)]\n",
    "    bins += [binary_gains.shape[0]]\n",
    "    bins = [binary_gains.iloc[bins[n]:bins[n + 1]] for n in range(n_bins)]\n",
    "    return all([bin.astype(bool).any() for bin in bins])\n",
    "\n",
    "def is_data_predictible(data, column):\n",
    "    return not is_null(data) & is_sparse(data, column)\n",
    "\n",
    "data.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate outlier sparsity of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(get_returns(data, column='close'))\n",
    "plt.show()\n",
    "is_data_predictible(data, 'close')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Percentage of the dataset generating rewards (keep between 5% to 15% or just rely on is_data_predictible())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(precalculate_ground_truths(data, column='close').iloc[:1000])\n",
    "plt.show()\n",
    "percent_rewardable = str(round(100 + precalculate_ground_truths(data, column='close').value_counts().pct_change().iloc[-1] * 100, 2)) + '%'\n",
    "print(percent_rewardable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threshold to pass to reward scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_test = pd.concat([X_train, X_test], axis='index')\n",
    "#threshold = estimate_percent_gains(X_train_test, 'close')\n",
    "threshold = estimate_percent_gains(X_train, 'close')\n",
    "threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement basic feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from feature_engine.selection import SelectBySingleFeaturePerformance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=10, \n",
    "                            random_state=seed, \n",
    "                            n_jobs=7)\n",
    "\n",
    "sel = SelectBySingleFeaturePerformance(variables=None, \n",
    "                                       estimator=rf, \n",
    "                                       scoring=\"roc_auc\", \n",
    "                                       cv=3, \n",
    "                                       threshold=0.50)\n",
    "\n",
    "sel.fit(X_train, precalculate_ground_truths(X_train, column='close'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_performance = pd.Series(sel.feature_performance_).sort_values(ascending=False)\n",
    "feature_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_performance.plot.bar(figsize=(20, 5))\n",
    "plt.title('Performance of ML models trained with individual features')\n",
    "plt.ylabel('roc-auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_drop = sel.features_to_drop_\n",
    "features_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = list(set(features_to_drop) - set(['open', 'high', 'low', 'close', 'volume']))\n",
    "len(to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop(columns=to_drop)\n",
    "X_test = X_test.drop(columns=to_drop)\n",
    "X_valid = X_valid.drop(columns=to_drop)\n",
    "\n",
    "X_train.shape, X_test.shape, X_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize the dataset subsets to make the model converge faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler\n",
    "\n",
    "scaler_type = MinMaxScaler\n",
    "\n",
    "def get_feature_scalers(X, scaler_type=scaler_type):\n",
    "    scalers = []\n",
    "    for name in list(X.columns[X.columns != 'date']):\n",
    "        scalers.append(scaler_type().fit(X[name].values.reshape(-1, 1)))\n",
    "    return scalers\n",
    "\n",
    "def get_scaler_transforms(X, scalers):\n",
    "    X_scaled = []\n",
    "    for name, scaler in zip(list(X.columns[X.columns != 'date']), scalers):\n",
    "        X_scaled.append(scaler.transform(X[name].values.reshape(-1, 1)))\n",
    "    X_scaled = pd.concat([pd.DataFrame(column, columns=[name]) for name, column in \\\n",
    "                          zip(list(X.columns[X.columns != 'date']), X_scaled)], axis='columns')\n",
    "    return X_scaled\n",
    "\n",
    "def normalize_data(X_train, X_test, X_valid):\n",
    "    X_train_test = pd.concat([X_train, X_test], axis='index')\n",
    "    X_train_test_valid = pd.concat([X_train_test, X_valid], axis='index')\n",
    "\n",
    "    X_train_test_dates = X_train_test[['date']]\n",
    "    X_train_test_valid_dates = X_train_test_valid[['date']]\n",
    "\n",
    "    X_train_test = X_train_test.drop(columns=['date'])\n",
    "    X_train_test_valid = X_train_test_valid.drop(columns=['date'])\n",
    "\n",
    "    train_test_scalers = \\\n",
    "        get_feature_scalers(X_train_test, \n",
    "                            scaler_type=scaler_type)\n",
    "    train_test_valid_scalers = \\\n",
    "        get_feature_scalers(X_train_test_valid, \n",
    "                            scaler_type=scaler_type)\n",
    "\n",
    "    X_train_test_scaled = \\\n",
    "        get_scaler_transforms(X_train_test, \n",
    "                              train_test_scalers)\n",
    "    X_train_test_valid_scaled = \\\n",
    "        get_scaler_transforms(X_train_test_valid, \n",
    "                              train_test_scalers)\n",
    "    X_train_test_valid_scaled_leaking = \\\n",
    "        get_scaler_transforms(X_train_test_valid, \n",
    "                              train_test_valid_scalers)\n",
    "\n",
    "    X_train_test_scaled = \\\n",
    "        pd.concat([X_train_test_dates, \n",
    "                   X_train_test_scaled], \n",
    "                  axis='columns')\n",
    "    X_train_test_valid_scaled = \\\n",
    "        pd.concat([X_train_test_valid_dates, \n",
    "                   X_train_test_valid_scaled], \n",
    "                  axis='columns')\n",
    "    X_train_test_valid_scaled_leaking = \\\n",
    "        pd.concat([X_train_test_valid_dates, \n",
    "                   X_train_test_valid_scaled_leaking], \n",
    "                  axis='columns')\n",
    "\n",
    "    X_train_scaled = X_train_test_scaled.iloc[:X_train.shape[0]]\n",
    "    X_test_scaled = X_train_test_scaled.iloc[X_train.shape[0]:]\n",
    "    X_valid_scaled = X_train_test_valid_scaled.iloc[X_train_test.shape[0]:]\n",
    "    X_valid_scaled_leaking = X_train_test_valid_scaled_leaking.iloc[X_train_test.shape[0]:]\n",
    "\n",
    "    return (train_test_scalers, \n",
    "            train_test_valid_scalers, \n",
    "            X_train_scaled, \n",
    "            X_test_scaled, \n",
    "            X_valid_scaled, \n",
    "            X_valid_scaled_leaking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_scalers, train_test_valid_scalers, X_train_scaled, X_test_scaled, X_valid_scaled, X_valid_scaled_leaking = \\\n",
    "    normalize_data(X_train, X_test, X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save new feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "\n",
    "train_csv = os.path.join(cwd, 'train.csv')\n",
    "test_csv = os.path.join(cwd, 'test.csv')\n",
    "valid_csv = os.path.join(cwd, 'valid.csv')\n",
    "train_scaled_csv = os.path.join(cwd, 'train_scaled.csv')\n",
    "test_scaled_csv = os.path.join(cwd, 'test_scaled.csv')\n",
    "valid_scaled_csv = os.path.join(cwd, 'valid_scaled.csv')\n",
    "valid_scaled_leaking_csv = os.path.join(cwd, 'valid_scaled_leaking.csv')\n",
    "\n",
    "#X_train.to_csv(train_csv, index=False)\n",
    "#X_test.to_csv(test_csv, index=False)\n",
    "#X_valid.to_csv(valid_csv, index=False)\n",
    "#X_train.to_csv(train_scaled_csv, index=False)\n",
    "#X_test.to_csv(test_scaled_csv, index=False)\n",
    "#X_valid.to_csv(valid_scaled_csv, index=False)\n",
    "#X_valid.to_csv(valid_scaled_leaking_csv, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write a renderer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Things to understand here:\n",
    "# Writing a Renderer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensortrade.env.generic import Renderer\n",
    "\n",
    "\n",
    "class PositionChangeChart(Renderer):\n",
    "    def __init__(self, color: str = \"orange\"):\n",
    "        self.color = \"orange\"\n",
    "\n",
    "    def render(self, env, **kwargs):\n",
    "        history = pd.DataFrame(env.observer.renderer_history)\n",
    "\n",
    "        actions = list(history.action)\n",
    "        price = list(history.close)\n",
    "\n",
    "        buy = {}\n",
    "        sell = {}\n",
    "\n",
    "        for i in range(len(actions) - 1):\n",
    "            a1 = actions[i]\n",
    "            a2 = actions[i + 1]\n",
    "\n",
    "            if a1 != a2:\n",
    "                if a1 == 0 and a2 == 1:\n",
    "                    buy[i] = price[i]\n",
    "                else:\n",
    "                    sell[i] = price[i]\n",
    "\n",
    "        buy = pd.Series(buy)\n",
    "        sell = pd.Series(sell)\n",
    "\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "        fig.suptitle(\"Performance\")\n",
    "\n",
    "        axs[0].plot(np.arange(len(price)), price, label=\"price\", color=self.color)\n",
    "        axs[0].scatter(buy.index, buy.values, marker=\"^\", color=\"green\")\n",
    "        axs[0].scatter(sell.index, sell.values, marker=\"^\", color=\"red\")\n",
    "        axs[0].set_title(\"Trading Chart\")\n",
    "\n",
    "        performance_df = pd.DataFrame().from_dict(env.action_scheme.portfolio.performance, orient='index')\n",
    "        performance_df.plot(ax=axs[1])\n",
    "        axs[1].set_title(\"Net Worth\")\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write a reward scheme encouraging rare volatile upside trades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensortrade.env.default.rewards import TensorTradeRewardScheme\n",
    "\n",
    "\n",
    "class PBR(TensorTradeRewardScheme):\n",
    "    \"\"\"A reward scheme for position-based returns.\n",
    "    * Let :math:`p_t` denote the price at time t.\n",
    "    * Let :math:`x_t` denote the position at time t.\n",
    "    * Let :math:`R_t` denote the reward at time t.\n",
    "    Then the reward is defined as,\n",
    "    :math:`R_{t} = (p_{t} - p_{t-1}) \\cdot x_{t}`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    price : `Stream`\n",
    "        The price stream to use for computing rewards.\n",
    "    \"\"\"\n",
    "\n",
    "    registered_name = \"pbr\"\n",
    "\n",
    "    def __init__(self, price: 'Stream', threshold: float = 0.02, window_size: int = 1) -> None:\n",
    "        super().__init__()\n",
    "        self._window_size = self.default('window_size', window_size)\n",
    "        self._threshold = self.default('threshold', threshold)\n",
    "        self.position = -1\n",
    "\n",
    "        position = Stream.sensor(self, lambda rs: rs.position, dtype=\"float\")\n",
    "        r = Stream.sensor(price, lambda p: p.value, dtype=\"float\").diff()\n",
    "        #r[(r > 0.0) & (r <= self._threshold)] = 0.0\n",
    "\n",
    "        reward = (position * r).fillna(0).rename(\"reward\")\n",
    "\n",
    "        self.feed = DataFeed([reward])\n",
    "        self.feed.compile()\n",
    "\n",
    "    def on_action(self, action: int) -> None:\n",
    "        self.position = -1 if action == 0 else 1\n",
    "\n",
    "    def get_reward(self, portfolio: 'Portfolio') -> float:\n",
    "        return self.feed.next()[\"reward\"]\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        \"\"\"Resets the `position` and `feed` of the reward scheme.\"\"\"\n",
    "        self.position = -1\n",
    "        self.feed.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnomalousProfit(TensorTradeRewardScheme):\n",
    "    \"\"\"A simple reward scheme that rewards the agent for exceeding a \n",
    "    precalculated percentage in the net worth.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    threshold : float\n",
    "        The minimum value to exceed in order to get the reward.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    threshold : float\n",
    "        The minimum value to exceed in order to get the reward.\n",
    "    \"\"\"\n",
    "\n",
    "    registered_name = \"anomalous\"\n",
    "\n",
    "    def __init__(self, threshold: float = 0.02, window_size: int = 1):\n",
    "        self._window_size = self.default('window_size', window_size)\n",
    "        self._threshold = self.default('threshold', threshold)\n",
    "\n",
    "    def get_reward(self, portfolio: 'Portfolio') -> float:\n",
    "        \"\"\"Rewards the agent for incremental increases in net worth over a\n",
    "        sliding window.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        portfolio : `Portfolio`\n",
    "            The portfolio being used by the environment.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            Whether the last percent change in net worth exceeds the predefined \n",
    "            `threshold`.\n",
    "        \"\"\"\n",
    "        performance = pd.DataFrame.from_dict(portfolio.performance).T\n",
    "        current_step = performance.shape[0]\n",
    "        if current_step > 1:\n",
    "            # Hint: make it cumulative.\n",
    "            net_worths = performance['net_worth']\n",
    "            ground_truths = precalculate_ground_truths(performance, \n",
    "                                                       column='net_worth', \n",
    "                                                       threshold=self._threshold)\n",
    "            reward_factor = 2.0 * ground_truths - 1.0\n",
    "            #return net_worths.iloc[-1] / net_worths.iloc[-min(current_step, self._window_size + 1)] - 1.0\n",
    "            return (reward_factor * net_worths.abs()).iloc[-1]\n",
    "\n",
    "        else:\n",
    "            return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PenalizedProfit(TensorTradeRewardScheme):\n",
    "    \"\"\"A reward scheme which penalizes net worth loss and \n",
    "    decays with the time spent.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cash_penalty_proportion : float\n",
    "        cash_penalty_proportion\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    cash_penalty_proportion : float\n",
    "        cash_penalty_proportion.\n",
    "    \"\"\"\n",
    "\n",
    "    registered_name = \"penalized\"\n",
    "\n",
    "    def __init__(self, cash_penalty_proportion: float = 0.10):\n",
    "        self._cash_penalty_proportion = \\\n",
    "            self.default('cash_penalty_proportion', \n",
    "                         cash_penalty_proportion)\n",
    "\n",
    "    def get_reward(self, portfolio: 'Portfolio') -> float:\n",
    "        \"\"\"Rewards the agent for gaining net worth while holding the asset.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        portfolio : `Portfolio`\n",
    "            The portfolio being used by the environment.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            A penalized reward.\n",
    "        \"\"\"\n",
    "        performance = pd.DataFrame.from_dict(portfolio.performance).T\n",
    "        current_step = performance.shape[0]\n",
    "        if current_step > 1:\n",
    "            initial_amount = portfolio.initial_net_worth\n",
    "            net_worth = performance['net_worth'].iloc[-1]\n",
    "            cash_worth = performance['bitstamp:/USD:/total'].iloc[-1]\n",
    "            cash_penalty = max(0, (net_worth * self._cash_penalty_proportion - cash_worth))\n",
    "            net_worth -= cash_penalty\n",
    "            reward = (net_worth / initial_amount) - 1\n",
    "            reward /= current_step\n",
    "            return reward\n",
    "        else:\n",
    "            return 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup trading environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_config_training = {\n",
    "    \"window_size\": 30,  # The number of past samples we want to look at (in hours)\n",
    "    \"max_allowed_loss\": 0.90,  # If it goes past 90% loss during the iteration, we don't want to waste time on a \"loser\".\n",
    "    \"data\": X_train,  # The variable that will be used to differentiate training and validation datasets\n",
    "    \"data_scaled\": X_train_scaled, \n",
    "    \"random_start_pct\": 0.0\n",
    "}\n",
    "\n",
    "env_config_evaluation = {\n",
    "    \"window_size\": 30,  # The number of past samples we want to look at (in hours)\n",
    "    \"max_allowed_loss\": 0.90,  # If it goes past 90% loss during the iteration, we don't want to waste time on a \"loser\".\n",
    "    \"data\": X_test,  # The variable that will be used to differentiate training and validation datasets\n",
    "    \"data_scaled\": X_test_scaled, \n",
    "    \"random_start_pct\": 0.0\n",
    "}\n",
    "\n",
    "env_config_validation = {\n",
    "    \"window_size\": 30,  # The number of past samples we want to look at (in hours)\n",
    "    \"max_allowed_loss\": 1.0,  # If it goes past 90% loss during the iteration, we don't want to waste time on a \"loser\".\n",
    "    \"data\": X_valid,  # The variable that will be used to differentiate training and validation datasets\n",
    "    \"data_scaled\": X_valid_scaled, \n",
    "    \"random_start_pct\": 0.0\n",
    "}\n",
    "\n",
    "X_unseen = pd.concat([X_test, X_valid], axis='index')\n",
    "X_unseen_scaled = pd.concat([X_test_scaled, X_valid_scaled_leaking], axis='index')\n",
    "\n",
    "env_config_unseen = {\n",
    "    \"window_size\": 30,  # The number of past samples we want to look at (in hours)\n",
    "    \"max_allowed_loss\": 1.0,  # If it goes past 90% loss during the iteration, we don't want to waste time on a \"loser\".\n",
    "    \"data\": X_unseen,  # The variable that will be used to differentiate training and validation datasets\n",
    "    \"data_scaled\": X_unseen_scaled, \n",
    "    \"random_start_pct\": 0.0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensortrade.env.default as default\n",
    "\n",
    "from tensortrade.feed.core import DataFeed, Stream\n",
    "from tensortrade.feed.core.base import NameSpace\n",
    "from tensortrade.env.default.actions import BSH\n",
    "from tensortrade.env.default.rewards import RiskAdjustedReturns, SimpleProfit\n",
    "from tensortrade.oms.exchanges import Exchange, ExchangeOptions\n",
    "from tensortrade.oms.services.execution.simulated import execute_order\n",
    "from tensortrade.oms.instruments import USD, BTC, ETH\n",
    "from tensortrade.oms.wallets import Wallet, Portfolio\n",
    "from tensortrade.oms.orders import TradeType\n",
    "\n",
    "def create_env(config):\n",
    "    # TODO: adjust according to your commission percentage, if present\n",
    "    commission = 0.001\n",
    "    price = Stream.source(list(config['data']['close']), \n",
    "                          dtype='float').rename('USD-BTC')\n",
    "    bitstamp_options = ExchangeOptions(commission=commission)\n",
    "    bitstamp = Exchange('bitstamp', \n",
    "                        service=execute_order, \n",
    "                        options=bitstamp_options)(price)\n",
    "\n",
    "    cash = Wallet(bitstamp, 10000 * USD)\n",
    "    asset = Wallet(bitstamp, 0 * BTC)\n",
    "\n",
    "    portfolio = Portfolio(USD, [cash, asset])\n",
    "\n",
    "    with NameSpace('bitstamp'):\n",
    "        features = [\n",
    "            Stream.source(list(config['data_scaled'][c]), \n",
    "                          dtype='float').rename(c) for c in config['data_scaled'].columns[1:]\n",
    "            #Stream.source(list(config['data_scaled']['lr_close']), dtype='float').rename('lr_close')\n",
    "        ]\n",
    "\n",
    "    feed = DataFeed(features)\n",
    "    feed.compile()\n",
    "\n",
    "    reward_scheme = PBR(price=price, threshold=threshold)\n",
    "\n",
    "    #reward_scheme = RiskAdjustedReturns(return_algorithm='sortino',\n",
    "    #                                    window_size=15)\n",
    "\n",
    "    #reward_scheme = SimpleProfit(window_size=30)\n",
    "\n",
    "    #reward_scheme = AnomalousProfit(threshold=threshold)\n",
    "\n",
    "    #reward_scheme = PenalizedProfit(cash_penalty_proportion=0.1)\n",
    "\n",
    "    #action_scheme = BSH(\n",
    "    #    cash=cash,\n",
    "    #    asset=asset\n",
    "    #)\n",
    "\n",
    "    action_scheme = BSH(\n",
    "        cash=cash,\n",
    "        asset=asset\n",
    "    ).attach(reward_scheme)\n",
    "\n",
    "    renderer_feed = DataFeed([\n",
    "        Stream.source(list(config['data']['date'])).rename('date'),\n",
    "        Stream.source(list(config['data']['open']), dtype='float').rename('open'),\n",
    "        Stream.source(list(config['data']['high']), dtype='float').rename('high'),\n",
    "        Stream.source(list(config['data']['low']), dtype='float').rename('low'),\n",
    "        Stream.source(list(config['data']['close']), dtype='float').rename('close'), \n",
    "        Stream.source(list(config['data']['volume']), dtype='float').rename('volume'), \n",
    "        Stream.sensor(action_scheme, \n",
    "                      lambda s: s.action, dtype='float').rename('action')\n",
    "    ])\n",
    "\n",
    "    renderer = [\n",
    "        PositionChangeChart(),\n",
    "        default.renderers.PlotlyTradingChart(),\n",
    "    ]\n",
    "\n",
    "    min_periods = config['window_size']  # Minimum of window_size\n",
    "\n",
    "    observer = default.observers.TensorTradeObserver(\n",
    "        portfolio=portfolio,\n",
    "        feed=feed,\n",
    "        renderer_feed=renderer_feed,\n",
    "        window_size=config['window_size'],\n",
    "        min_periods=min_periods\n",
    "    )\n",
    "\n",
    "    stopper = default.stoppers.MaxLossStopper(\n",
    "        max_allowed_loss=config['max_allowed_loss']\n",
    "    )\n",
    "\n",
    "    informer = default.informers.TensorTradeInformer()\n",
    "\n",
    "    env = default.create(\n",
    "        portfolio=portfolio,\n",
    "        action_scheme=action_scheme,\n",
    "        reward_scheme=reward_scheme,\n",
    "        feed=feed,\n",
    "        renderer_feed=renderer_feed,\n",
    "        renderer=renderer,\n",
    "        observer=observer,\n",
    "        stopper=stopper,\n",
    "        informer=informer,\n",
    "        min_periods=min_periods,\n",
    "        random_start_pct=config['random_start_pct'],\n",
    "        window_size=config['window_size']\n",
    "    )\n",
    "\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = create_env(config=env_config_training)\n",
    "env.observer.feed.next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimal_batch_size(window_size=30, n_steps=1000, batch_factor=4, stride=1):\n",
    "    \"\"\"\n",
    "    lookback = 30          # Days of past data (also named window_size).\n",
    "    batch_factor = 4       # batch_size = (sample_size - lookback - stride) // batch_factor\n",
    "    stride = 1             # Time series shift into the future.\n",
    "    \"\"\"\n",
    "    lookback = window_size\n",
    "    sample_size = n_steps\n",
    "    batch_size = ((sample_size - lookback - stride) // batch_factor)\n",
    "    return batch_size\n",
    "\n",
    "batch_size = get_optimal_batch_size(window_size=window_size, n_steps=n_steps, batch_factor=4)\n",
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --bind_all --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: implement tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensortrade.agents as agents\n",
    "from tensortrade.agents import A2C\n",
    "from tensortrade.agents.utils.common import ModelReader, create_envs\n",
    "\n",
    "envs = create_envs(env)\n",
    "model = ModelReader(\n",
    "    agents.agents['a2c']['model']['cnn'][0],\n",
    "    output_units=[envs[0].action_space.n, 1],\n",
    "    input_shape=envs[0].observation_space.shape,\n",
    "    optimizer='adam',\n",
    ").build_model()\n",
    "agent = A2C(envs, model, preprocess=False)\n",
    "agent.fit(target_reward=19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env = create_env(config=env_config_unseen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.play(render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
